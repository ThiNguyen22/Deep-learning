{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ebb2c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01280943]\n",
      " [0.98909273]\n",
      " [0.98909413]\n",
      " [0.01119434]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Hàm kích hoạt sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Lớp mạng nơ-ron\n",
    "class XORNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Khởi tạo các trọng số ngẫu nhiên\n",
    "        self.weights1 = np.random.randn(2, 2)  # Trọng số lớp ẩn\n",
    "        self.bias1 = np.zeros((1, 2))          # Bias lớp ẩn\n",
    "        self.weights2 = np.random.randn(2, 1)  # Trọng số lớp đầu ra\n",
    "        self.bias2 = np.zeros((1, 1))          # Bias lớp đầu ra\n",
    "    \n",
    "    def feedforward(self, X):\n",
    "        # Lan truyền tiến\n",
    "        self.hidden_sum = np.dot(X, self.weights1) + self.bias1\n",
    "        self.hidden_output = sigmoid(self.hidden_sum)\n",
    "        self.output_sum = np.dot(self.hidden_output, self.weights2) + self.bias2\n",
    "        self.output = sigmoid(self.output_sum)\n",
    "        return self.output\n",
    "    \n",
    "    def backpropagation(self, X, y, output):\n",
    "        # Lan truyền ngược để cập nhật lại trọng số và bias\n",
    "        self.error = y - output\n",
    "        self.d_output = self.error * sigmoid(self.output_sum) * (1 - sigmoid(self.output_sum))\n",
    "        \n",
    "        self.hidden_error = np.dot(self.d_output, self.weights2.T)\n",
    "        self.d_hidden = self.hidden_error * sigmoid(self.hidden_sum) * (1 - sigmoid(self.hidden_sum))\n",
    "        \n",
    "        self.weights2 += np.dot(self.hidden_output.T, self.d_output)\n",
    "        self.bias2 += np.sum(self.d_output, axis=0, keepdims=True)\n",
    "        self.weights1 += np.dot(X.T, self.d_hidden)\n",
    "        self.bias1 += np.sum(self.d_hidden, axis=0)\n",
    "    \n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.feedforward(X)\n",
    "            self.backpropagation(X, y, output)\n",
    "\n",
    "# Dữ liệu đầu vào và đầu ra XOR\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "# Khởi tạo mạng nơ-ron XOR\n",
    "nn = XORNeuralNetwork()\n",
    "\n",
    "# Huấn luyện mạng nơ-ron\n",
    "nn.train(X, y, epochs=10000)\n",
    "\n",
    "# Dự đoán đầu ra cho dữ liệu đầu vào\n",
    "predictions = nn.feedforward(X)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f910ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05383564]\n",
      " [0.9487817 ]\n",
      " [0.94891361]\n",
      " [0.05612059]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Hàm kích hoạt sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Lớp mạng nơ-ron\n",
    "class XORNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Khởi tạo các trọng số ngẫu nhiên\n",
    "        self.weights1 = np.random.randn(2, 2)  # Trọng số lớp ẩn\n",
    "        self.bias1 = np.zeros((1, 2))          # Bias lớp ẩn\n",
    "        self.weights2 = np.random.randn(2, 1)  # Trọng số lớp đầu ra\n",
    "        self.bias2 = np.zeros((1, 1))          # Bias lớp đầu ra\n",
    "    \n",
    "    def feedforward(self, X):\n",
    "        # Lan truyền tiến\n",
    "        self.hidden_sum = np.dot(X, self.weights1) + self.bias1\n",
    "        self.hidden_output = sigmoid(self.hidden_sum)\n",
    "        self.output_sum = np.dot(self.hidden_output, self.weights2) + self.bias2\n",
    "        self.output = sigmoid(self.output_sum)\n",
    "        return self.output\n",
    "    \n",
    "    def backpropagation(self, X, y, learning_rate):\n",
    "        # Lan truyền ngược và cập nhật trọng số/bias dựa trên gradient descent\n",
    "        output = self.feedforward(X)\n",
    "        error = y - output\n",
    "        \n",
    "        # Đạo hàm của hàm mất mát đối với các tham số\n",
    "        d_output = error * sigmoid(self.output_sum) * (1 - sigmoid(self.output_sum))\n",
    "        d_hidden = np.dot(d_output, self.weights2.T) * sigmoid(self.hidden_sum) * (1 - sigmoid(self.hidden_sum))\n",
    "        \n",
    "        # Cập nhật trọng số và bias\n",
    "        self.weights2 += learning_rate * np.dot(self.hidden_output.T, d_output)\n",
    "        self.bias2 += learning_rate * np.sum(d_output, axis=0, keepdims=True)\n",
    "        self.weights1 += learning_rate * np.dot(X.T, d_hidden)\n",
    "        self.bias1 += learning_rate * np.sum(d_hidden, axis=0)\n",
    "    \n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            self.backpropagation(X, y, learning_rate)\n",
    "\n",
    "# Dữ liệu đầu vào và đầu ra XOR\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "# Khởi tạo mạng nơ-ron XOR\n",
    "nn = XORNeuralNetwork()\n",
    "\n",
    "# Huấn luyện mạng nơ-ron sử dụng gradient descent\n",
    "nn.train(X, y, epochs=10000, learning_rate=0.1)\n",
    "\n",
    "# Dự đoán đầu ra cho dữ liệu đầu vào\n",
    "predictions = nn.feedforward(X)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e0e88d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100000 Loss: 0.6963601732126478 \n",
      "Epoch 200/100000 Loss: 0.6955195701274778 \n",
      "Epoch 300/100000 Loss: 0.6947873385082067 \n",
      "Epoch 400/100000 Loss: 0.6940726059515336 \n",
      "Epoch 500/100000 Loss: 0.6932915973720388 \n",
      "Epoch 600/100000 Loss: 0.692343617057624 \n",
      "Epoch 700/100000 Loss: 0.691103618665433 \n",
      "Epoch 800/100000 Loss: 0.6894253508356843 \n",
      "Epoch 900/100000 Loss: 0.6871628663195294 \n",
      "Epoch 1000/100000 Loss: 0.6842056006094135 \n",
      "Epoch 1100/100000 Loss: 0.6804989693846708 \n",
      "Epoch 1200/100000 Loss: 0.6760230787031286 \n",
      "Epoch 1300/100000 Loss: 0.6707426813376398 \n",
      "Epoch 1400/100000 Loss: 0.664569633696079 \n",
      "Epoch 1500/100000 Loss: 0.6573611822307847 \n",
      "Epoch 1600/100000 Loss: 0.6489510033847259 \n",
      "Epoch 1700/100000 Loss: 0.6392021169058828 \n",
      "Epoch 1800/100000 Loss: 0.6280684888417245 \n",
      "Epoch 1900/100000 Loss: 0.6156438058133864 \n",
      "Epoch 2000/100000 Loss: 0.6021716216035816 \n",
      "Epoch 2100/100000 Loss: 0.588006302537095 \n",
      "Epoch 2200/100000 Loss: 0.5735422930676485 \n",
      "Epoch 2300/100000 Loss: 0.5591442623873405 \n",
      "Epoch 2400/100000 Loss: 0.5451020044750569 \n",
      "Epoch 2500/100000 Loss: 0.5316153686037032 \n",
      "Epoch 2600/100000 Loss: 0.5188010306705723 \n",
      "Epoch 2700/100000 Loss: 0.5067085290356214 \n",
      "Epoch 2800/100000 Loss: 0.4953350155283892 \n",
      "Epoch 2900/100000 Loss: 0.48463231985738003 \n",
      "Epoch 3000/100000 Loss: 0.47450246061438184 \n",
      "Epoch 3100/100000 Loss: 0.464776101737777 \n",
      "Epoch 3200/100000 Loss: 0.45515994803572374 \n",
      "Epoch 3300/100000 Loss: 0.4451176241650815 \n",
      "Epoch 3400/100000 Loss: 0.433604241362365 \n",
      "Epoch 3500/100000 Loss: 0.41854662556591443 \n",
      "Epoch 3600/100000 Loss: 0.3964462638396979 \n",
      "Epoch 3700/100000 Loss: 0.36481808630548335 \n",
      "Epoch 3800/100000 Loss: 0.3277633350251288 \n",
      "Epoch 3900/100000 Loss: 0.29182111998747284 \n",
      "Epoch 4000/100000 Loss: 0.25920763517496936 \n",
      "Epoch 4100/100000 Loss: 0.2300565486729348 \n",
      "Epoch 4200/100000 Loss: 0.2043506652643358 \n",
      "Epoch 4300/100000 Loss: 0.18200413216777594 \n",
      "Epoch 4400/100000 Loss: 0.16278085912077697 \n",
      "Epoch 4500/100000 Loss: 0.1463353421575917 \n",
      "Epoch 4600/100000 Loss: 0.13228412668549883 \n",
      "Epoch 4700/100000 Loss: 0.12025730981228697 \n",
      "Epoch 4800/100000 Loss: 0.10992450838416223 \n",
      "Epoch 4900/100000 Loss: 0.10100324869669172 \n",
      "Epoch 5000/100000 Loss: 0.0932578925804235 \n",
      "Epoch 5100/100000 Loss: 0.08649444035324366 \n",
      "Epoch 5200/100000 Loss: 0.08055413437800388 \n",
      "Epoch 5300/100000 Loss: 0.07530725773063776 \n",
      "Epoch 5400/100000 Loss: 0.07064768386240637 \n",
      "Epoch 5500/100000 Loss: 0.06648831715362134 \n",
      "Epoch 5600/100000 Loss: 0.06275737912293755 \n",
      "Epoch 5700/100000 Loss: 0.05939542667877464 \n",
      "Epoch 5800/100000 Loss: 0.05635297523536479 \n",
      "Epoch 5900/100000 Loss: 0.05358860911979546 \n",
      "Epoch 6000/100000 Loss: 0.051067479159478554 \n",
      "Epoch 6100/100000 Loss: 0.04876010574283647 \n",
      "Epoch 6200/100000 Loss: 0.046641422204186586 \n",
      "Epoch 6300/100000 Loss: 0.04469000725333495 \n",
      "Epoch 6400/100000 Loss: 0.042887466357522566 \n",
      "Epoch 6500/100000 Loss: 0.04121793081811592 \n",
      "Epoch 6600/100000 Loss: 0.03966765017912486 \n",
      "Epoch 6700/100000 Loss: 0.038224658951704064 \n",
      "Epoch 6800/100000 Loss: 0.03687850277448376 \n",
      "Epoch 6900/100000 Loss: 0.035620012327248436 \n",
      "Epoch 7000/100000 Loss: 0.034441115790961215 \n",
      "Epoch 7100/100000 Loss: 0.0333346825680068 \n",
      "Epoch 7200/100000 Loss: 0.03229439247160715 \n",
      "Epoch 7300/100000 Loss: 0.03131462576120256 \n",
      "Epoch 7400/100000 Loss: 0.030390370316367838 \n",
      "Epoch 7500/100000 Loss: 0.029517142962899257 \n",
      "Epoch 7600/100000 Loss: 0.02869092253488088 \n",
      "Epoch 7700/100000 Loss: 0.02790809270931534 \n",
      "Epoch 7800/100000 Loss: 0.027165393011010334 \n",
      "Epoch 7900/100000 Loss: 0.0264598766746413 \n",
      "Epoch 8000/100000 Loss: 0.025788874283544226 \n",
      "Epoch 8100/100000 Loss: 0.025149962292690387 \n",
      "Epoch 8200/100000 Loss: 0.024540935695671985 \n",
      "Epoch 8300/100000 Loss: 0.023959784219602813 \n",
      "Epoch 8400/100000 Loss: 0.02340467153324815 \n",
      "Epoch 8500/100000 Loss: 0.022873917036911422 \n",
      "Epoch 8600/100000 Loss: 0.022365979871132622 \n",
      "Epoch 8700/100000 Loss: 0.021879444837890012 \n",
      "Epoch 8800/100000 Loss: 0.02141300997496913 \n",
      "Epoch 8900/100000 Loss: 0.020965475563259933 \n",
      "Epoch 9000/100000 Loss: 0.020535734379376617 \n",
      "Epoch 9100/100000 Loss: 0.02012276303333934 \n",
      "Epoch 9200/100000 Loss: 0.019725614254025386 \n",
      "Epoch 9300/100000 Loss: 0.019343410004456666 \n",
      "Epoch 9400/100000 Loss: 0.018975335325355502 \n",
      "Epoch 9500/100000 Loss: 0.018620632819264855 \n",
      "Epoch 9600/100000 Loss: 0.018278597699323797 \n",
      "Epoch 9700/100000 Loss: 0.0179485733368284 \n",
      "Epoch 9800/100000 Loss: 0.0176299472502978 \n",
      "Epoch 9900/100000 Loss: 0.01732214748611369 \n",
      "Epoch 10000/100000 Loss: 0.017024639347118342 \n",
      "Epoch 10100/100000 Loss: 0.016736922430987945 \n",
      "Epoch 10200/100000 Loss: 0.01645852794488913 \n",
      "Epoch 10300/100000 Loss: 0.016189016266978558 \n",
      "Epoch 10400/100000 Loss: 0.01592797472881869 \n",
      "Epoch 10500/100000 Loss: 0.01567501559583395 \n",
      "Epoch 10600/100000 Loss: 0.015429774225585842 \n",
      "Epoch 10700/100000 Loss: 0.015191907385958386 \n",
      "Epoch 10800/100000 Loss: 0.014961091717370858 \n",
      "Epoch 10900/100000 Loss: 0.014737022324901462 \n",
      "Epoch 11000/100000 Loss: 0.014519411487757847 \n",
      "Epoch 11100/100000 Loss: 0.014307987474896883 \n",
      "Epoch 11200/100000 Loss: 0.014102493456791439 \n",
      "Epoch 11300/100000 Loss: 0.013902686504403999 \n",
      "Epoch 11400/100000 Loss: 0.013708336667357848 \n",
      "Epoch 11500/100000 Loss: 0.013519226124124118 \n",
      "Epoch 11600/100000 Loss: 0.013335148397774667 \n",
      "Epoch 11700/100000 Loss: 0.013155907631499774 \n",
      "Epoch 11800/100000 Loss: 0.012981317918667447 \n",
      "Epoch 11900/100000 Loss: 0.012811202682711879 \n",
      "Epoch 12000/100000 Loss: 0.012645394102600413 \n",
      "Epoch 12100/100000 Loss: 0.012483732580033196 \n",
      "Epoch 12200/100000 Loss: 0.012326066244894893 \n",
      "Epoch 12300/100000 Loss: 0.012172250495807429 \n",
      "Epoch 12400/100000 Loss: 0.01202214757291999 \n",
      "Epoch 12500/100000 Loss: 0.011875626160341667 \n",
      "Epoch 12600/100000 Loss: 0.011732561015850223 \n",
      "Epoch 12700/100000 Loss: 0.011592832625730097 \n",
      "Epoch 12800/100000 Loss: 0.01145632688277767 \n",
      "Epoch 12900/100000 Loss: 0.011322934785687999 \n",
      "Epoch 13000/100000 Loss: 0.01119255215818923 \n",
      "Epoch 13100/100000 Loss: 0.011065079386435854 \n",
      "Epoch 13200/100000 Loss: 0.010940421173292459 \n",
      "Epoch 13300/100000 Loss: 0.010818486308262074 \n",
      "Epoch 13400/100000 Loss: 0.010699187451909961 \n",
      "Epoch 13500/100000 Loss: 0.01058244093373554 \n",
      "Epoch 13600/100000 Loss: 0.010468166562524271 \n",
      "Epoch 13700/100000 Loss: 0.010356287448294664 \n",
      "Epoch 13800/100000 Loss: 0.010246729835024157 \n",
      "Epoch 13900/100000 Loss: 0.01013942294340199 \n",
      "Epoch 14000/100000 Loss: 0.01003429882291817 \n",
      "Epoch 14100/100000 Loss: 0.009931292212649913 \n",
      "Epoch 14200/100000 Loss: 0.009830340410156235 \n",
      "Epoch 14300/100000 Loss: 0.009731383147937566 \n",
      "Epoch 14400/100000 Loss: 0.009634362476957464 \n",
      "Epoch 14500/100000 Loss: 0.009539222656761149 \n",
      "Epoch 14600/100000 Loss: 0.009445910051760292 \n",
      "Epoch 14700/100000 Loss: 0.009354373033286088 \n",
      "Epoch 14800/100000 Loss: 0.009264561887040581 \n",
      "Epoch 14900/100000 Loss: 0.009176428725602526 \n",
      "Epoch 15000/100000 Loss: 0.009089927405671641 \n",
      "Epoch 15100/100000 Loss: 0.009005013449753105 \n",
      "Epoch 15200/100000 Loss: 0.008921643972009792 \n",
      "Epoch 15300/100000 Loss: 0.008839777608024813 \n",
      "Epoch 15400/100000 Loss: 0.008759374448237866 \n",
      "Epoch 15500/100000 Loss: 0.008680395974832646 \n",
      "Epoch 15600/100000 Loss: 0.008602805001870972 \n",
      "Epoch 15700/100000 Loss: 0.008526565618477712 \n",
      "Epoch 15800/100000 Loss: 0.0084516431349013 \n",
      "Epoch 15900/100000 Loss: 0.008378004031277751 \n",
      "Epoch 16000/100000 Loss: 0.008305615908945425 \n",
      "Epoch 16100/100000 Loss: 0.008234447444162185 \n",
      "Epoch 16200/100000 Loss: 0.008164468344087454 \n",
      "Epoch 16300/100000 Loss: 0.00809564930490404 \n",
      "Epoch 16400/100000 Loss: 0.008027961971956387 \n",
      "Epoch 16500/100000 Loss: 0.00796137890179615 \n",
      "Epoch 16600/100000 Loss: 0.007895873526026727 \n",
      "Epoch 16700/100000 Loss: 0.007831420116852193 \n",
      "Epoch 16800/100000 Loss: 0.007767993754233477 \n",
      "Epoch 16900/100000 Loss: 0.007705570294568739 \n",
      "Epoch 17000/100000 Loss: 0.007644126340814882 \n",
      "Epoch 17100/100000 Loss: 0.00758363921397308 \n",
      "Epoch 17200/100000 Loss: 0.007524086925867658 \n",
      "Epoch 17300/100000 Loss: 0.007465448153149928 \n",
      "Epoch 17400/100000 Loss: 0.007407702212462583 \n",
      "Epoch 17500/100000 Loss: 0.0073508290367068334 \n",
      "Epoch 17600/100000 Loss: 0.007294809152353811 \n",
      "Epoch 17700/100000 Loss: 0.007239623657747328 \n",
      "Epoch 17800/100000 Loss: 0.007185254202349674 \n",
      "Epoch 17900/100000 Loss: 0.007131682966880876 \n",
      "Epoch 18000/100000 Loss: 0.007078892644308534 \n",
      "Epoch 18100/100000 Loss: 0.007026866421645328 \n",
      "Epoch 18200/100000 Loss: 0.006975587962514979 \n",
      "Epoch 18300/100000 Loss: 0.006925041390449029 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18400/100000 Loss: 0.006875211272878779 \n",
      "Epoch 18500/100000 Loss: 0.006826082605789782 \n",
      "Epoch 18600/100000 Loss: 0.006777640799005734 \n",
      "Epoch 18700/100000 Loss: 0.006729871662073803 \n",
      "Epoch 18800/100000 Loss: 0.006682761390720896 \n",
      "Epoch 18900/100000 Loss: 0.006636296553856292 \n",
      "Epoch 19000/100000 Loss: 0.006590464081093566 \n",
      "Epoch 19100/100000 Loss: 0.006545251250768434 \n",
      "Epoch 19200/100000 Loss: 0.006500645678429661 \n",
      "Epoch 19300/100000 Loss: 0.006456635305781455 \n",
      "Epoch 19400/100000 Loss: 0.006413208390057308 \n",
      "Epoch 19500/100000 Loss: 0.006370353493804253 \n",
      "Epoch 19600/100000 Loss: 0.00632805947506194 \n",
      "Epoch 19700/100000 Loss: 0.006286315477915499 \n",
      "Epoch 19800/100000 Loss: 0.00624511092340907 \n",
      "Epoch 19900/100000 Loss: 0.006204435500801454 \n",
      "Epoch 20000/100000 Loss: 0.0061642791591503335 \n",
      "Epoch 20100/100000 Loss: 0.006124632099211047 \n",
      "Epoch 20200/100000 Loss: 0.006085484765634892 \n",
      "Epoch 20300/100000 Loss: 0.006046827839455411 \n",
      "Epoch 20400/100000 Loss: 0.006008652230850153 \n",
      "Epoch 20500/100000 Loss: 0.005970949072165931 \n",
      "Epoch 20600/100000 Loss: 0.0059337097111959655 \n",
      "Epoch 20700/100000 Loss: 0.005896925704700473 \n",
      "Epoch 20800/100000 Loss: 0.005860588812157218 \n",
      "Epoch 20900/100000 Loss: 0.005824690989736085 \n",
      "Epoch 21000/100000 Loss: 0.0057892243844851286 \n",
      "Epoch 21100/100000 Loss: 0.005754181328722589 \n",
      "Epoch 21200/100000 Loss: 0.005719554334623742 \n",
      "Epoch 21300/100000 Loss: 0.00568533608899679 \n",
      "Epoch 21400/100000 Loss: 0.005651519448238549 \n",
      "Epoch 21500/100000 Loss: 0.005618097433463985 \n",
      "Epoch 21600/100000 Loss: 0.005585063225802006 \n",
      "Epoch 21700/100000 Loss: 0.005552410161851588 \n",
      "Epoch 21800/100000 Loss: 0.0055201317292904286 \n",
      "Epoch 21900/100000 Loss: 0.0054882215626322825 \n",
      "Epoch 22000/100000 Loss: 0.00545667343912541 \n",
      "Epoch 22100/100000 Loss: 0.005425481274788199 \n",
      "Epoch 22200/100000 Loss: 0.005394639120574198 \n",
      "Epoch 22300/100000 Loss: 0.005364141158665538 \n",
      "Epoch 22400/100000 Loss: 0.005333981698885474 \n",
      "Epoch 22500/100000 Loss: 0.005304155175229166 \n",
      "Epoch 22600/100000 Loss: 0.005274656142505874 \n",
      "Epoch 22700/100000 Loss: 0.005245479273089515 \n",
      "Epoch 22800/100000 Loss: 0.005216619353773292 \n",
      "Epoch 22900/100000 Loss: 0.0051880712827244175 \n",
      "Epoch 23000/100000 Loss: 0.005159830066535231 \n",
      "Epoch 23100/100000 Loss: 0.0051318908173678904 \n",
      "Epoch 23200/100000 Loss: 0.005104248750187728 \n",
      "Epoch 23300/100000 Loss: 0.005076899180083435 \n",
      "Epoch 23400/100000 Loss: 0.005049837519670316 \n",
      "Epoch 23500/100000 Loss: 0.00502305927657412 \n",
      "Epoch 23600/100000 Loss: 0.004996560050991059 \n",
      "Epoch 23700/100000 Loss: 0.004970335533324148 \n",
      "Epoch 23800/100000 Loss: 0.004944381501889783 \n",
      "Epoch 23900/100000 Loss: 0.004918693820694762 \n",
      "Epoch 24000/100000 Loss: 0.004893268437279989 \n",
      "Epoch 24100/100000 Loss: 0.004868101380628648 \n",
      "Epoch 24200/100000 Loss: 0.00484318875913705 \n",
      "Epoch 24300/100000 Loss: 0.0048185267586453836 \n",
      "Epoch 24400/100000 Loss: 0.004794111640526789 \n",
      "Epoch 24500/100000 Loss: 0.004769939739832938 \n",
      "Epoch 24600/100000 Loss: 0.004746007463493129 \n",
      "Epoch 24700/100000 Loss: 0.004722311288566375 \n",
      "Epoch 24800/100000 Loss: 0.00469884776054405 \n",
      "Epoch 24900/100000 Loss: 0.004675613491701329 \n",
      "Epoch 25000/100000 Loss: 0.004652605159495752 \n",
      "Epoch 25100/100000 Loss: 0.004629819505012307 \n",
      "Epoch 25200/100000 Loss: 0.00460725333145203 \n",
      "Epoch 25300/100000 Loss: 0.004584903502663019 \n",
      "Epoch 25400/100000 Loss: 0.004562766941714244 \n",
      "Epoch 25500/100000 Loss: 0.004540840629507871 \n",
      "Epoch 25600/100000 Loss: 0.004519121603431147 \n",
      "Epoch 25700/100000 Loss: 0.004497606956045111 \n",
      "Epoch 25800/100000 Loss: 0.00447629383381081 \n",
      "Epoch 25900/100000 Loss: 0.004455179435848801 \n",
      "Epoch 26000/100000 Loss: 0.004434261012734202 \n",
      "Epoch 26100/100000 Loss: 0.004413535865323543 \n",
      "Epoch 26200/100000 Loss: 0.004393001343614544 \n",
      "Epoch 26300/100000 Loss: 0.004372654845635381 \n",
      "Epoch 26400/100000 Loss: 0.00435249381636527 \n",
      "Epoch 26500/100000 Loss: 0.004332515746682903 \n",
      "Epoch 26600/100000 Loss: 0.004312718172343435 \n",
      "Epoch 26700/100000 Loss: 0.004293098672982741 \n",
      "Epoch 26800/100000 Loss: 0.004273654871147565 \n",
      "Epoch 26900/100000 Loss: 0.004254384431351869 \n",
      "Epoch 27000/100000 Loss: 0.004235285059157027 \n",
      "Epoch 27100/100000 Loss: 0.00421635450027703 \n",
      "Epoch 27200/100000 Loss: 0.004197590539706333 \n",
      "Epoch 27300/100000 Loss: 0.004178991000870226 \n",
      "Epoch 27400/100000 Loss: 0.004160553744797659 \n",
      "Epoch 27500/100000 Loss: 0.00414227666931495 \n",
      "Epoch 27600/100000 Loss: 0.004124157708259647 \n",
      "Epoch 27700/100000 Loss: 0.004106194830715433 \n",
      "Epoch 27800/100000 Loss: 0.004088386040265626 \n",
      "Epoch 27900/100000 Loss: 0.004070729374265644 \n",
      "Epoch 28000/100000 Loss: 0.004053222903134253 \n",
      "Epoch 28100/100000 Loss: 0.0040358647296615865 \n",
      "Epoch 28200/100000 Loss: 0.004018652988335663 \n",
      "Epoch 28300/100000 Loss: 0.004001585844684702 \n",
      "Epoch 28400/100000 Loss: 0.003984661494635732 \n",
      "Epoch 28500/100000 Loss: 0.003967878163889802 \n",
      "Epoch 28600/100000 Loss: 0.003951234107311389 \n",
      "Epoch 28700/100000 Loss: 0.0039347276083340245 \n",
      "Epoch 28800/100000 Loss: 0.003918356978378716 \n",
      "Epoch 28900/100000 Loss: 0.0039021205562881694 \n",
      "Epoch 29000/100000 Loss: 0.0038860167077736443 \n",
      "Epoch 29100/100000 Loss: 0.0038700438248756193 \n",
      "Epoch 29200/100000 Loss: 0.003854200325436575 \n",
      "Epoch 29300/100000 Loss: 0.003838484652587387 \n",
      "Epoch 29400/100000 Loss: 0.0038228952742455466 \n",
      "Epoch 29500/100000 Loss: 0.0038074306826247285 \n",
      "Epoch 29600/100000 Loss: 0.003792089393756596 \n",
      "Epoch 29700/100000 Loss: 0.0037768699470236925 \n",
      "Epoch 29800/100000 Loss: 0.0037617709047030887 \n",
      "Epoch 29900/100000 Loss: 0.0037467908515205796 \n",
      "Epoch 30000/100000 Loss: 0.003731928394215766 \n",
      "Epoch 30100/100000 Loss: 0.0037171821611165427 \n",
      "Epoch 30200/100000 Loss: 0.0037025508017236516 \n",
      "Epoch 30300/100000 Loss: 0.0036880329863050554 \n",
      "Epoch 30400/100000 Loss: 0.0036736274054991434 \n",
      "Epoch 30500/100000 Loss: 0.0036593327699273913 \n",
      "Epoch 30600/100000 Loss: 0.0036451478098149842 \n",
      "Epoch 30700/100000 Loss: 0.0036310712746215816 \n",
      "Epoch 30800/100000 Loss: 0.0036171019326786498 \n",
      "Epoch 30900/100000 Loss: 0.003603238570835845 \n",
      "Epoch 31000/100000 Loss: 0.003589479994115713 \n",
      "Epoch 31100/100000 Loss: 0.0035758250253748996 \n",
      "Epoch 31200/100000 Loss: 0.003562272504974008 \n",
      "Epoch 31300/100000 Loss: 0.003548821290453991 \n",
      "Epoch 31400/100000 Loss: 0.0035354702562200415 \n",
      "Epoch 31500/100000 Loss: 0.003522218293232602 \n",
      "Epoch 31600/100000 Loss: 0.003509064308704695 \n",
      "Epoch 31700/100000 Loss: 0.0034960072258058686 \n",
      "Epoch 31800/100000 Loss: 0.00348304598337299 \n",
      "Epoch 31900/100000 Loss: 0.003470179535627117 \n",
      "Epoch 32000/100000 Loss: 0.0034574068518963045 \n",
      "Epoch 32100/100000 Loss: 0.003444726916344387 \n",
      "Epoch 32200/100000 Loss: 0.003432138727705871 \n",
      "Epoch 32300/100000 Loss: 0.003419641299026132 \n",
      "Epoch 32400/100000 Loss: 0.003407233657407573 \n",
      "Epoch 32500/100000 Loss: 0.00339491484376014 \n",
      "Epoch 32600/100000 Loss: 0.0033826839125588298 \n",
      "Epoch 32700/100000 Loss: 0.0033705399316047687 \n",
      "Epoch 32800/100000 Loss: 0.0033584819817915794 \n",
      "Epoch 32900/100000 Loss: 0.0033465091568776932 \n",
      "Epoch 33000/100000 Loss: 0.003334620563261904 \n",
      "Epoch 33100/100000 Loss: 0.0033228153197643697 \n",
      "Epoch 33200/100000 Loss: 0.0033110925574127174 \n",
      "Epoch 33300/100000 Loss: 0.0032994514192308897 \n",
      "Epoch 33400/100000 Loss: 0.0032878910600341606 \n",
      "Epoch 33500/100000 Loss: 0.0032764106462272674 \n",
      "Epoch 33600/100000 Loss: 0.0032650093556067105 \n",
      "Epoch 33700/100000 Loss: 0.0032536863771675632 \n",
      "Epoch 33800/100000 Loss: 0.003242440910913914 \n",
      "Epoch 33900/100000 Loss: 0.003231272167672981 \n",
      "Epoch 34000/100000 Loss: 0.0032201793689133054 \n",
      "Epoch 34100/100000 Loss: 0.0032091617465660857 \n",
      "Epoch 34200/100000 Loss: 0.0031982185428511417 \n",
      "Epoch 34300/100000 Loss: 0.003187349010104801 \n",
      "Epoch 34400/100000 Loss: 0.0031765524106126828 \n",
      "Epoch 34500/100000 Loss: 0.003165828016444688 \n",
      "Epoch 34600/100000 Loss: 0.003155175109294018 \n",
      "Epoch 34700/100000 Loss: 0.0031445929803189552 \n",
      "Epoch 34800/100000 Loss: 0.0031340809299877557 \n",
      "Epoch 34900/100000 Loss: 0.0031236382679268268 \n",
      "Epoch 35000/100000 Loss: 0.003113264312771572 \n",
      "Epoch 35100/100000 Loss: 0.0031029583920202954 \n",
      "Epoch 35200/100000 Loss: 0.003092719841890971 \n",
      "Epoch 35300/100000 Loss: 0.003082548007180535 \n",
      "Epoch 35400/100000 Loss: 0.0030724422411274074 \n",
      "Epoch 35500/100000 Loss: 0.003062401905275844 \n",
      "Epoch 35600/100000 Loss: 0.003052426369343616 \n",
      "Epoch 35700/100000 Loss: 0.0030425150110918895 \n",
      "Epoch 35800/100000 Loss: 0.003032667216197766 \n",
      "Epoch 35900/100000 Loss: 0.003022882378128873 \n",
      "Epoch 36000/100000 Loss: 0.0030131598980204246 \n",
      "Epoch 36100/100000 Loss: 0.0030034991845551444 \n",
      "Epoch 36200/100000 Loss: 0.002993899653844812 \n",
      "Epoch 36300/100000 Loss: 0.002984360729314088 \n",
      "Epoch 36400/100000 Loss: 0.0029748818415867794 \n",
      "Epoch 36500/100000 Loss: 0.0029654624283741472 \n",
      "Epoch 36600/100000 Loss: 0.0029561019343649626 \n",
      "Epoch 36700/100000 Loss: 0.00294679981111833 \n",
      "Epoch 36800/100000 Loss: 0.0029375555169575598 \n",
      "Epoch 36900/100000 Loss: 0.0029283685168664184 \n",
      "Epoch 37000/100000 Loss: 0.0029192382823878746 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37100/100000 Loss: 0.002910164291523437 \n",
      "Epoch 37200/100000 Loss: 0.0029011460286354116 \n",
      "Epoch 37300/100000 Loss: 0.0028921829843504155 \n",
      "Epoch 37400/100000 Loss: 0.0028832746554648867 \n",
      "Epoch 37500/100000 Loss: 0.002874420544851632 \n",
      "Epoch 37600/100000 Loss: 0.0028656201613694544 \n",
      "Epoch 37700/100000 Loss: 0.0028568730197727887 \n",
      "Epoch 37800/100000 Loss: 0.002848178640624077 \n",
      "Epoch 37900/100000 Loss: 0.00283953655020722 \n",
      "Epoch 38000/100000 Loss: 0.0028309462804426986 \n",
      "Epoch 38100/100000 Loss: 0.0028224073688041104 \n",
      "Epoch 38200/100000 Loss: 0.0028139193582361827 \n",
      "Epoch 38300/100000 Loss: 0.0028054817970746392 \n",
      "Epoch 38400/100000 Loss: 0.0027970942389667713 \n",
      "Epoch 38500/100000 Loss: 0.0027887562427938543 \n",
      "Epoch 38600/100000 Loss: 0.0027804673725948304 \n",
      "Epoch 38700/100000 Loss: 0.0027722271974914728 \n",
      "Epoch 38800/100000 Loss: 0.0027640352916143947 \n",
      "Epoch 38900/100000 Loss: 0.0027558912340308246 \n",
      "Epoch 39000/100000 Loss: 0.002747794608673443 \n",
      "Epoch 39100/100000 Loss: 0.0027397450042703168 \n",
      "Epoch 39200/100000 Loss: 0.0027317420142763185 \n",
      "Epoch 39300/100000 Loss: 0.0027237852368055503 \n",
      "Epoch 39400/100000 Loss: 0.0027158742745646744 \n",
      "Epoch 39500/100000 Loss: 0.002708008734787998 \n",
      "Epoch 39600/100000 Loss: 0.0027001882291731827 \n",
      "Epoch 39700/100000 Loss: 0.0026924123738178516 \n",
      "Epoch 39800/100000 Loss: 0.002684680789158178 \n",
      "Epoch 39900/100000 Loss: 0.0026769930999073167 \n",
      "Epoch 40000/100000 Loss: 0.0026693489349957893 \n",
      "Epoch 40100/100000 Loss: 0.002661747927512599 \n",
      "Epoch 40200/100000 Loss: 0.002654189714646719 \n",
      "Epoch 40300/100000 Loss: 0.002646673937631112 \n",
      "Epoch 40400/100000 Loss: 0.0026392002416853457 \n",
      "Epoch 40500/100000 Loss: 0.0026317682759619656 \n",
      "Epoch 40600/100000 Loss: 0.002624377693490732 \n",
      "Epoch 40700/100000 Loss: 0.0026170281511267365 \n",
      "Epoch 40800/100000 Loss: 0.002609719309496985 \n",
      "Epoch 40900/100000 Loss: 0.002602450832949082 \n",
      "Epoch 41000/100000 Loss: 0.002595222389500808 \n",
      "Epoch 41100/100000 Loss: 0.002588033650789587 \n",
      "Epoch 41200/100000 Loss: 0.0025808842920240853 \n",
      "Epoch 41300/100000 Loss: 0.0025737739919352745 \n",
      "Epoch 41400/100000 Loss: 0.002566702432729287 \n",
      "Epoch 41500/100000 Loss: 0.0025596693000406686 \n",
      "Epoch 41600/100000 Loss: 0.002552674282885937 \n",
      "Epoch 41700/100000 Loss: 0.0025457170736190677 \n",
      "Epoch 41800/100000 Loss: 0.0025387973678860382 \n",
      "Epoch 41900/100000 Loss: 0.0025319148645818004 \n",
      "Epoch 42000/100000 Loss: 0.0025250692658066658 \n",
      "Epoch 42100/100000 Loss: 0.0025182602768240433 \n",
      "Epoch 42200/100000 Loss: 0.002511487606018529 \n",
      "Epoch 42300/100000 Loss: 0.00250475096485493 \n",
      "Epoch 42400/100000 Loss: 0.002498050067837536 \n",
      "Epoch 42500/100000 Loss: 0.0024913846324707634 \n",
      "Epoch 42600/100000 Loss: 0.0024847543792193235 \n",
      "Epoch 42700/100000 Loss: 0.0024781590314700737 \n",
      "Epoch 42800/100000 Loss: 0.0024715983154939207 \n",
      "Epoch 42900/100000 Loss: 0.002465071960408146 \n",
      "Epoch 43000/100000 Loss: 0.002458579698140103 \n",
      "Epoch 43100/100000 Loss: 0.0024521212633903566 \n",
      "Epoch 43200/100000 Loss: 0.0024456963935973568 \n",
      "Epoch 43300/100000 Loss: 0.002439304828902288 \n",
      "Epoch 43400/100000 Loss: 0.0024329463121142477 \n",
      "Epoch 43500/100000 Loss: 0.002426620588676133 \n",
      "Epoch 43600/100000 Loss: 0.002420327406631521 \n",
      "Epoch 43700/100000 Loss: 0.0024140665165911213 \n",
      "Epoch 43800/100000 Loss: 0.002407837671700442 \n",
      "Epoch 43900/100000 Loss: 0.0024016406276077757 \n",
      "Epoch 44000/100000 Loss: 0.0023954751424324025 \n",
      "Epoch 44100/100000 Loss: 0.002389340976733777 \n",
      "Epoch 44200/100000 Loss: 0.0023832378934807415 \n",
      "Epoch 44300/100000 Loss: 0.0023771656580213285 \n",
      "Epoch 44400/100000 Loss: 0.0023711240380532034 \n",
      "Epoch 44500/100000 Loss: 0.0023651128035941144 \n",
      "Epoch 44600/100000 Loss: 0.0023591317269533976 \n",
      "Epoch 44700/100000 Loss: 0.0023531805827032913 \n",
      "Epoch 44800/100000 Loss: 0.0023472591476510086 \n",
      "Epoch 44900/100000 Loss: 0.0023413672008114443 \n",
      "Epoch 45000/100000 Loss: 0.002335504523379362 \n",
      "Epoch 45100/100000 Loss: 0.0023296708987033347 \n",
      "Epoch 45200/100000 Loss: 0.0023238661122590193 \n",
      "Epoch 45300/100000 Loss: 0.002318089951623073 \n",
      "Epoch 45400/100000 Loss: 0.0023123422064478256 \n",
      "Epoch 45500/100000 Loss: 0.002306622668435728 \n",
      "Epoch 45600/100000 Loss: 0.002300931131314782 \n",
      "Epoch 45700/100000 Loss: 0.0022952673908138224 \n",
      "Epoch 45800/100000 Loss: 0.0022896312446385137 \n",
      "Epoch 45900/100000 Loss: 0.0022840224924474415 \n",
      "Epoch 46000/100000 Loss: 0.0022784409358284994 \n",
      "Epoch 46100/100000 Loss: 0.0022728863782761864 \n",
      "Epoch 46200/100000 Loss: 0.0022673586251683803 \n",
      "Epoch 46300/100000 Loss: 0.0022618574837440916 \n",
      "Epoch 46400/100000 Loss: 0.0022563827630812374 \n",
      "Epoch 46500/100000 Loss: 0.0022509342740749828 \n",
      "Epoch 46600/100000 Loss: 0.002245511829415745 \n",
      "Epoch 46700/100000 Loss: 0.0022401152435686748 \n",
      "Epoch 46800/100000 Loss: 0.002234744332752112 \n",
      "Epoch 46900/100000 Loss: 0.0022293989149173203 \n",
      "Epoch 47000/100000 Loss: 0.0022240788097278898 \n",
      "Epoch 47100/100000 Loss: 0.0022187838385401737 \n",
      "Epoch 47200/100000 Loss: 0.0022135138243825816 \n",
      "Epoch 47300/100000 Loss: 0.002208268591937325 \n",
      "Epoch 47400/100000 Loss: 0.002203047967520162 \n",
      "Epoch 47500/100000 Loss: 0.0021978517790620616 \n",
      "Epoch 47600/100000 Loss: 0.002192679856090007 \n",
      "Epoch 47700/100000 Loss: 0.0021875320297090805 \n",
      "Epoch 47800/100000 Loss: 0.0021824081325839944 \n",
      "Epoch 47900/100000 Loss: 0.0021773079989212338 \n",
      "Epoch 48000/100000 Loss: 0.002172231464451201 \n",
      "Epoch 48100/100000 Loss: 0.002167178366411059 \n",
      "Epoch 48200/100000 Loss: 0.0021621485435275173 \n",
      "Epoch 48300/100000 Loss: 0.002157141835999507 \n",
      "Epoch 48400/100000 Loss: 0.0021521580854818117 \n",
      "Epoch 48500/100000 Loss: 0.0021471971350684642 \n",
      "Epoch 48600/100000 Loss: 0.0021422588292763487 \n",
      "Epoch 48700/100000 Loss: 0.002137343014029247 \n",
      "Epoch 48800/100000 Loss: 0.002132449536642078 \n",
      "Epoch 48900/100000 Loss: 0.002127578245805084 \n",
      "Epoch 49000/100000 Loss: 0.00212272899156844 \n",
      "Epoch 49100/100000 Loss: 0.0021179016253271062 \n",
      "Epoch 49200/100000 Loss: 0.0021130959998056607 \n",
      "Epoch 49300/100000 Loss: 0.002108311969043909 \n",
      "Epoch 49400/100000 Loss: 0.0021035493883817136 \n",
      "Epoch 49500/100000 Loss: 0.0020988081144449455 \n",
      "Epoch 49600/100000 Loss: 0.002094088005131121 \n",
      "Epoch 49700/100000 Loss: 0.002089388919595211 \n",
      "Epoch 49800/100000 Loss: 0.0020847107182362604 \n",
      "Epoch 49900/100000 Loss: 0.0020800532626832507 \n",
      "Epoch 50000/100000 Loss: 0.002075416415781667 \n",
      "Epoch 50100/100000 Loss: 0.002070800041580337 \n",
      "Epoch 50200/100000 Loss: 0.002066204005318442 \n",
      "Epoch 50300/100000 Loss: 0.0020616281734119443 \n",
      "Epoch 50400/100000 Loss: 0.0020570724134414313 \n",
      "Epoch 50500/100000 Loss: 0.002052536594139159 \n",
      "Epoch 50600/100000 Loss: 0.002048020585376702 \n",
      "Epoch 50700/100000 Loss: 0.002043524258152387 \n",
      "Epoch 50800/100000 Loss: 0.0020390474845795874 \n",
      "Epoch 50900/100000 Loss: 0.002034590137874433 \n",
      "Epoch 51000/100000 Loss: 0.0020301520923437467 \n",
      "Epoch 51100/100000 Loss: 0.0020257332233739518 \n",
      "Epoch 51200/100000 Loss: 0.0020213334074190934 \n",
      "Epoch 51300/100000 Loss: 0.0020169525219894133 \n",
      "Epoch 51400/100000 Loss: 0.0020125904456405417 \n",
      "Epoch 51500/100000 Loss: 0.002008247057961933 \n",
      "Epoch 51600/100000 Loss: 0.00200392223956606 \n",
      "Epoch 51700/100000 Loss: 0.0019996158720776047 \n",
      "Epoch 51800/100000 Loss: 0.0019953278381228986 \n",
      "Epoch 51900/100000 Loss: 0.001991058021319203 \n",
      "Epoch 52000/100000 Loss: 0.001986806306264177 \n",
      "Epoch 52100/100000 Loss: 0.0019825725785259933 \n",
      "Epoch 52200/100000 Loss: 0.001978356724632835 \n",
      "Epoch 52300/100000 Loss: 0.001974158632062959 \n",
      "Epoch 52400/100000 Loss: 0.0019699781892348034 \n",
      "Epoch 52500/100000 Loss: 0.00196581528549708 \n",
      "Epoch 52600/100000 Loss: 0.0019616698111195006 \n",
      "Epoch 52700/100000 Loss: 0.0019575416572829166 \n",
      "Epoch 52800/100000 Loss: 0.00195343071606972 \n",
      "Epoch 52900/100000 Loss: 0.0019493368804551511 \n",
      "Epoch 53000/100000 Loss: 0.0019452600442973092 \n",
      "Epoch 53100/100000 Loss: 0.0019412001023287415 \n",
      "Epoch 53200/100000 Loss: 0.0019371569501473432 \n",
      "Epoch 53300/100000 Loss: 0.001933130484207115 \n",
      "Epoch 53400/100000 Loss: 0.0019291206018097584 \n",
      "Epoch 53500/100000 Loss: 0.001925127201096269 \n",
      "Epoch 53600/100000 Loss: 0.0019211501810376707 \n",
      "Epoch 53700/100000 Loss: 0.0019171894414272493 \n",
      "Epoch 53800/100000 Loss: 0.0019132448828719283 \n",
      "Epoch 53900/100000 Loss: 0.001909316406784252 \n",
      "Epoch 54000/100000 Loss: 0.001905403915373791 \n",
      "Epoch 54100/100000 Loss: 0.00190150731163971 \n",
      "Epoch 54200/100000 Loss: 0.0018976264993624805 \n",
      "Epoch 54300/100000 Loss: 0.0018937613830960318 \n",
      "Epoch 54400/100000 Loss: 0.0018899118681600753 \n",
      "Epoch 54500/100000 Loss: 0.0018860778606327336 \n",
      "Epoch 54600/100000 Loss: 0.0018822592673422766 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54700/100000 Loss: 0.0018784559958605605 \n",
      "Epoch 54800/100000 Loss: 0.0018746679544947087 \n",
      "Epoch 54900/100000 Loss: 0.0018708950522806625 \n",
      "Epoch 55000/100000 Loss: 0.001867137198975282 \n",
      "Epoch 55100/100000 Loss: 0.0018633943050498664 \n",
      "Epoch 55200/100000 Loss: 0.0018596662816825638 \n",
      "Epoch 55300/100000 Loss: 0.001855953040751643 \n",
      "Epoch 55400/100000 Loss: 0.0018522544948287355 \n",
      "Epoch 55500/100000 Loss: 0.0018485705571718038 \n",
      "Epoch 55600/100000 Loss: 0.0018449011417186912 \n",
      "Epoch 55700/100000 Loss: 0.0018412461630801721 \n",
      "Epoch 55800/100000 Loss: 0.0018376055365337258 \n",
      "Epoch 55900/100000 Loss: 0.0018339791780167549 \n",
      "Epoch 56000/100000 Loss: 0.0018303670041204983 \n",
      "Epoch 56100/100000 Loss: 0.0018267689320834181 \n",
      "Epoch 56200/100000 Loss: 0.0018231848797850565 \n",
      "Epoch 56300/100000 Loss: 0.001819614765739868 \n",
      "Epoch 56400/100000 Loss: 0.0018160585090911622 \n",
      "Epoch 56500/100000 Loss: 0.0018125160296048792 \n",
      "Epoch 56600/100000 Loss: 0.0018089872476637022 \n",
      "Epoch 56700/100000 Loss: 0.0018054720842612477 \n",
      "Epoch 56800/100000 Loss: 0.0018019704609961791 \n",
      "Epoch 56900/100000 Loss: 0.0017984823000663473 \n",
      "Epoch 57000/100000 Loss: 0.0017950075242630914 \n",
      "Epoch 57100/100000 Loss: 0.0017915460569658567 \n",
      "Epoch 57200/100000 Loss: 0.0017880978221361922 \n",
      "Epoch 57300/100000 Loss: 0.0017846627443126157 \n",
      "Epoch 57400/100000 Loss: 0.0017812407486048915 \n",
      "Epoch 57500/100000 Loss: 0.0017778317606889247 \n",
      "Epoch 57600/100000 Loss: 0.0017744357068010654 \n",
      "Epoch 57700/100000 Loss: 0.0017710525137332263 \n",
      "Epoch 57800/100000 Loss: 0.001767682108827441 \n",
      "Epoch 57900/100000 Loss: 0.0017643244199708663 \n",
      "Epoch 58000/100000 Loss: 0.0017609793755901488 \n",
      "Epoch 58100/100000 Loss: 0.0017576469046474289 \n",
      "Epoch 58200/100000 Loss: 0.0017543269366344558 \n",
      "Epoch 58300/100000 Loss: 0.0017510194015679832 \n",
      "Epoch 58400/100000 Loss: 0.0017477242299850225 \n",
      "Epoch 58500/100000 Loss: 0.0017444413529376555 \n",
      "Epoch 58600/100000 Loss: 0.0017411707019885673 \n",
      "Epoch 58700/100000 Loss: 0.0017379122092066918 \n",
      "Epoch 58800/100000 Loss: 0.0017346658071616065 \n",
      "Epoch 58900/100000 Loss: 0.0017314314289195957 \n",
      "Epoch 59000/100000 Loss: 0.0017282090080391838 \n",
      "Epoch 59100/100000 Loss: 0.0017249984785663395 \n",
      "Epoch 59200/100000 Loss: 0.0017217997750297856 \n",
      "Epoch 59300/100000 Loss: 0.0017186128324373994 \n",
      "Epoch 59400/100000 Loss: 0.001715437586270828 \n",
      "Epoch 59500/100000 Loss: 0.0017122739724821957 \n",
      "Epoch 59600/100000 Loss: 0.0017091219274889408 \n",
      "Epoch 59700/100000 Loss: 0.0017059813881702456 \n",
      "Epoch 59800/100000 Loss: 0.0017028522918625143 \n",
      "Epoch 59900/100000 Loss: 0.0016997345763554133 \n",
      "Epoch 60000/100000 Loss: 0.0016966281798876273 \n",
      "Epoch 60100/100000 Loss: 0.0016935330411429281 \n",
      "Epoch 60200/100000 Loss: 0.0016904490992464044 \n",
      "Epoch 60300/100000 Loss: 0.0016873762937598615 \n",
      "Epoch 60400/100000 Loss: 0.001684314564678694 \n",
      "Epoch 60500/100000 Loss: 0.0016812638524274508 \n",
      "Epoch 60600/100000 Loss: 0.001678224097856208 \n",
      "Epoch 60700/100000 Loss: 0.0016751952422368573 \n",
      "Epoch 60800/100000 Loss: 0.0016721772272592279 \n",
      "Epoch 60900/100000 Loss: 0.0016691699950275442 \n",
      "Epoch 61000/100000 Loss: 0.001666173488056409 \n",
      "Epoch 61100/100000 Loss: 0.0016631876492674816 \n",
      "Epoch 61200/100000 Loss: 0.0016602124219861564 \n",
      "Epoch 61300/100000 Loss: 0.0016572477499372993 \n",
      "Epoch 61400/100000 Loss: 0.0016542935772421194 \n",
      "Epoch 61500/100000 Loss: 0.0016513498484148777 \n",
      "Epoch 61600/100000 Loss: 0.0016484165083591488 \n",
      "Epoch 61700/100000 Loss: 0.0016454935023645853 \n",
      "Epoch 61800/100000 Loss: 0.0016425807761032906 \n",
      "Epoch 61900/100000 Loss: 0.0016396782756269738 \n",
      "Epoch 62000/100000 Loss: 0.001636785947363016 \n",
      "Epoch 62100/100000 Loss: 0.0016339037381117635 \n",
      "Epoch 62200/100000 Loss: 0.0016310315950430981 \n",
      "Epoch 62300/100000 Loss: 0.001628169465693061 \n",
      "Epoch 62400/100000 Loss: 0.0016253172979609509 \n",
      "Epoch 62500/100000 Loss: 0.0016224750401060335 \n",
      "Epoch 62600/100000 Loss: 0.0016196426407445554 \n",
      "Epoch 62700/100000 Loss: 0.0016168200488465379 \n",
      "Epoch 62800/100000 Loss: 0.0016140072137328738 \n",
      "Epoch 62900/100000 Loss: 0.0016112040850722329 \n",
      "Epoch 63000/100000 Loss: 0.0016084106128781601 \n",
      "Epoch 63100/100000 Loss: 0.0016056267475060347 \n",
      "Epoch 63200/100000 Loss: 0.0016028524396501983 \n",
      "Epoch 63300/100000 Loss: 0.00160008764034122 \n",
      "Epoch 63400/100000 Loss: 0.0015973323009427177 \n",
      "Epoch 63500/100000 Loss: 0.0015945863731490692 \n",
      "Epoch 63600/100000 Loss: 0.0015918498089818164 \n",
      "Epoch 63700/100000 Loss: 0.001589122560787683 \n",
      "Epoch 63800/100000 Loss: 0.0015864045812353116 \n",
      "Epoch 63900/100000 Loss: 0.0015836958233129209 \n",
      "Epoch 64000/100000 Loss: 0.001580996240325349 \n",
      "Epoch 64100/100000 Loss: 0.00157830578589132 \n",
      "Epoch 64200/100000 Loss: 0.001575624413941269 \n",
      "Epoch 64300/100000 Loss: 0.001572952078714328 \n",
      "Epoch 64400/100000 Loss: 0.0015702887347559301 \n",
      "Epoch 64500/100000 Loss: 0.0015676343369149066 \n",
      "Epoch 64600/100000 Loss: 0.001564988840341619 \n",
      "Epoch 64700/100000 Loss: 0.0015623522004849187 \n",
      "Epoch 64800/100000 Loss: 0.001559724373089833 \n",
      "Epoch 64900/100000 Loss: 0.00155710531419511 \n",
      "Epoch 65000/100000 Loss: 0.0015544949801307368 \n",
      "Epoch 65100/100000 Loss: 0.0015518933275157074 \n",
      "Epoch 65200/100000 Loss: 0.001549300313255402 \n",
      "Epoch 65300/100000 Loss: 0.0015467158945395789 \n",
      "Epoch 65400/100000 Loss: 0.0015441400288394743 \n",
      "Epoch 65500/100000 Loss: 0.0015415726739059616 \n",
      "Epoch 65600/100000 Loss: 0.0015390137877673178 \n",
      "Epoch 65700/100000 Loss: 0.001536463328726551 \n",
      "Epoch 65800/100000 Loss: 0.0015339212553593894 \n",
      "Epoch 65900/100000 Loss: 0.001531387526512133 \n",
      "Epoch 66000/100000 Loss: 0.0015288621012993142 \n",
      "Epoch 66100/100000 Loss: 0.0015263449391016027 \n",
      "Epoch 66200/100000 Loss: 0.0015238359995634115 \n",
      "Epoch 66300/100000 Loss: 0.0015213352425913584 \n",
      "Epoch 66400/100000 Loss: 0.0015188426283513413 \n",
      "Epoch 66500/100000 Loss: 0.001516358117267059 \n",
      "Epoch 66600/100000 Loss: 0.0015138816700177517 \n",
      "Epoch 66700/100000 Loss: 0.001511413247536055 \n",
      "Epoch 66800/100000 Loss: 0.0015089528110059929 \n",
      "Epoch 66900/100000 Loss: 0.0015065003218611337 \n",
      "Epoch 67000/100000 Loss: 0.0015040557417822244 \n",
      "Epoch 67100/100000 Loss: 0.0015016190326957095 \n",
      "Epoch 67200/100000 Loss: 0.0014991901567714169 \n",
      "Epoch 67300/100000 Loss: 0.0014967690764205804 \n",
      "Epoch 67400/100000 Loss: 0.001494355754294303 \n",
      "Epoch 67500/100000 Loss: 0.001491950153281161 \n",
      "Epoch 67600/100000 Loss: 0.001489552236505614 \n",
      "Epoch 67700/100000 Loss: 0.001487161967326108 \n",
      "Epoch 67800/100000 Loss: 0.00148477930933332 \n",
      "Epoch 67900/100000 Loss: 0.0014824042263479537 \n",
      "Epoch 68000/100000 Loss: 0.0014800366824194305 \n",
      "Epoch 68100/100000 Loss: 0.0014776766418236587 \n",
      "Epoch 68200/100000 Loss: 0.0014753240690613316 \n",
      "Epoch 68300/100000 Loss: 0.0014729789288566446 \n",
      "Epoch 68400/100000 Loss: 0.0014706411861548437 \n",
      "Epoch 68500/100000 Loss: 0.0014683108061208853 \n",
      "Epoch 68600/100000 Loss: 0.0014659877541375967 \n",
      "Epoch 68700/100000 Loss: 0.0014636719958042812 \n",
      "Epoch 68800/100000 Loss: 0.0014613634969343496 \n",
      "Epoch 68900/100000 Loss: 0.0014590622235545938 \n",
      "Epoch 69000/100000 Loss: 0.0014567681419026506 \n",
      "Epoch 69100/100000 Loss: 0.001454481218425886 \n",
      "Epoch 69200/100000 Loss: 0.001452201419779555 \n",
      "Epoch 69300/100000 Loss: 0.001449928712825464 \n",
      "Epoch 69400/100000 Loss: 0.0014476630646299343 \n",
      "Epoch 69500/100000 Loss: 0.0014454044424626878 \n",
      "Epoch 69600/100000 Loss: 0.0014431528137949512 \n",
      "Epoch 69700/100000 Loss: 0.0014409081462980616 \n",
      "Epoch 69800/100000 Loss: 0.0014386704078418779 \n",
      "Epoch 69900/100000 Loss: 0.0014364395664933298 \n",
      "Epoch 70000/100000 Loss: 0.0014342155905148301 \n",
      "Epoch 70100/100000 Loss: 0.0014319984483630193 \n",
      "Epoch 70200/100000 Loss: 0.0014297881086866757 \n",
      "Epoch 70300/100000 Loss: 0.001427584540326016 \n",
      "Epoch 70400/100000 Loss: 0.0014253877123107483 \n",
      "Epoch 70500/100000 Loss: 0.0014231975938587558 \n",
      "Epoch 70600/100000 Loss: 0.0014210141543749312 \n",
      "Epoch 70700/100000 Loss: 0.0014188373634491944 \n",
      "Epoch 70800/100000 Loss: 0.0014166671908556012 \n",
      "Epoch 70900/100000 Loss: 0.0014145036065506165 \n",
      "Epoch 71000/100000 Loss: 0.001412346580672246 \n",
      "Epoch 71100/100000 Loss: 0.0014101960835380062 \n",
      "Epoch 71200/100000 Loss: 0.0014080520856439723 \n",
      "Epoch 71300/100000 Loss: 0.001405914557663554 \n",
      "Epoch 71400/100000 Loss: 0.0014037834704456835 \n",
      "Epoch 71500/100000 Loss: 0.0014016587950139501 \n",
      "Epoch 71600/100000 Loss: 0.001399540502565374 \n",
      "Epoch 71700/100000 Loss: 0.0013974285644684282 \n",
      "Epoch 71800/100000 Loss: 0.0013953229522625918 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71900/100000 Loss: 0.0013932236376565923 \n",
      "Epoch 72000/100000 Loss: 0.0013911305925271818 \n",
      "Epoch 72100/100000 Loss: 0.0013890437889181877 \n",
      "Epoch 72200/100000 Loss: 0.0013869631990388676 \n",
      "Epoch 72300/100000 Loss: 0.0013848887952630745 \n",
      "Epoch 72400/100000 Loss: 0.001382820550127694 \n",
      "Epoch 72500/100000 Loss: 0.00138075843633156 \n",
      "Epoch 72600/100000 Loss: 0.0013787024267347815 \n",
      "Epoch 72700/100000 Loss: 0.0013766524943563223 \n",
      "Epoch 72800/100000 Loss: 0.001374608612374359 \n",
      "Epoch 72900/100000 Loss: 0.0013725707541238593 \n",
      "Epoch 73000/100000 Loss: 0.00137053889309619 \n",
      "Epoch 73100/100000 Loss: 0.0013685130029376402 \n",
      "Epoch 73200/100000 Loss: 0.001366493057448306 \n",
      "Epoch 73300/100000 Loss: 0.001364479030581199 \n",
      "Epoch 73400/100000 Loss: 0.0013624708964408525 \n",
      "Epoch 73500/100000 Loss: 0.0013604686292824296 \n",
      "Epoch 73600/100000 Loss: 0.0013584722035104692 \n",
      "Epoch 73700/100000 Loss: 0.001356481593677965 \n",
      "Epoch 73800/100000 Loss: 0.0013544967744851966 \n",
      "Epoch 73900/100000 Loss: 0.0013525177207784464 \n",
      "Epoch 74000/100000 Loss: 0.0013505444075496919 \n",
      "Epoch 74100/100000 Loss: 0.001348576809934602 \n",
      "Epoch 74200/100000 Loss: 0.0013466149032121157 \n",
      "Epoch 74300/100000 Loss: 0.001344658662803329 \n",
      "Epoch 74400/100000 Loss: 0.0013427080642702406 \n",
      "Epoch 74500/100000 Loss: 0.0013407630833149153 \n",
      "Epoch 74600/100000 Loss: 0.0013388236957786474 \n",
      "Epoch 74700/100000 Loss: 0.0013368898776406242 \n",
      "Epoch 74800/100000 Loss: 0.0013349616050170893 \n",
      "Epoch 74900/100000 Loss: 0.0013330388541605335 \n",
      "Epoch 75000/100000 Loss: 0.0013311216014585253 \n",
      "Epoch 75100/100000 Loss: 0.0013292098234328465 \n",
      "Epoch 75200/100000 Loss: 0.001327303496738239 \n",
      "Epoch 75300/100000 Loss: 0.0013254025981620406 \n",
      "Epoch 75400/100000 Loss: 0.0013235071046229876 \n",
      "Epoch 75500/100000 Loss: 0.001321616993169878 \n",
      "Epoch 75600/100000 Loss: 0.0013197322409812633 \n",
      "Epoch 75700/100000 Loss: 0.0013178528253643901 \n",
      "Epoch 75800/100000 Loss: 0.0013159787237538632 \n",
      "Epoch 75900/100000 Loss: 0.0013141099137114773 \n",
      "Epoch 76000/100000 Loss: 0.001312246372924517 \n",
      "Epoch 76100/100000 Loss: 0.001310388079205646 \n",
      "Epoch 76200/100000 Loss: 0.0013085350104915135 \n",
      "Epoch 76300/100000 Loss: 0.0013066871448420576 \n",
      "Epoch 76400/100000 Loss: 0.001304844460439779 \n",
      "Epoch 76500/100000 Loss: 0.001303006935588407 \n",
      "Epoch 76600/100000 Loss: 0.001301174548712785 \n",
      "Epoch 76700/100000 Loss: 0.0012993472783576717 \n",
      "Epoch 76800/100000 Loss: 0.0012975251031864072 \n",
      "Epoch 76900/100000 Loss: 0.0012957080019811037 \n",
      "Epoch 77000/100000 Loss: 0.0012938959536409767 \n",
      "Epoch 77100/100000 Loss: 0.0012920889371821195 \n",
      "Epoch 77200/100000 Loss: 0.0012902869317360016 \n",
      "Epoch 77300/100000 Loss: 0.0012884899165496876 \n",
      "Epoch 77400/100000 Loss: 0.0012866978709839463 \n",
      "Epoch 77500/100000 Loss: 0.0012849107745133027 \n",
      "Epoch 77600/100000 Loss: 0.00128312860672476 \n",
      "Epoch 77700/100000 Loss: 0.0012813513473173498 \n",
      "Epoch 77800/100000 Loss: 0.0012795789761011881 \n",
      "Epoch 77900/100000 Loss: 0.0012778114729967485 \n",
      "Epoch 78000/100000 Loss: 0.0012760488180340273 \n",
      "Epoch 78100/100000 Loss: 0.001274290991352097 \n",
      "Epoch 78200/100000 Loss: 0.0012725379731979088 \n",
      "Epoch 78300/100000 Loss: 0.0012707897439260968 \n",
      "Epoch 78400/100000 Loss: 0.0012690462839977529 \n",
      "Epoch 78500/100000 Loss: 0.0012673075739799526 \n",
      "Epoch 78600/100000 Loss: 0.0012655735945451124 \n",
      "Epoch 78700/100000 Loss: 0.00126384432646988 \n",
      "Epoch 78800/100000 Loss: 0.0012621197506351553 \n",
      "Epoch 78900/100000 Loss: 0.0012603998480246203 \n",
      "Epoch 79000/100000 Loss: 0.001258684599724456 \n",
      "Epoch 79100/100000 Loss: 0.0012569739869226755 \n",
      "Epoch 79200/100000 Loss: 0.0012552679909082318 \n",
      "Epoch 79300/100000 Loss: 0.001253566593070489 \n",
      "Epoch 79400/100000 Loss: 0.001251869774898497 \n",
      "Epoch 79500/100000 Loss: 0.0012501775179804629 \n",
      "Epoch 79600/100000 Loss: 0.0012484898040028025 \n",
      "Epoch 79700/100000 Loss: 0.0012468066147498077 \n",
      "Epoch 79800/100000 Loss: 0.001245127932102698 \n",
      "Epoch 79900/100000 Loss: 0.0012434537380392853 \n",
      "Epoch 80000/100000 Loss: 0.001241784014632946 \n",
      "Epoch 80100/100000 Loss: 0.0012401187440525326 \n",
      "Epoch 80200/100000 Loss: 0.0012384579085610698 \n",
      "Epoch 80300/100000 Loss: 0.00123680149051575 \n",
      "Epoch 80400/100000 Loss: 0.0012351494723668488 \n",
      "Epoch 80500/100000 Loss: 0.0012335018366575294 \n",
      "Epoch 80600/100000 Loss: 0.001231858566022701 \n",
      "Epoch 80700/100000 Loss: 0.0012302196431891283 \n",
      "Epoch 80800/100000 Loss: 0.0012285850509740415 \n",
      "Epoch 80900/100000 Loss: 0.0012269547722851054 \n",
      "Epoch 81000/100000 Loss: 0.0012253287901198078 \n",
      "Epoch 81100/100000 Loss: 0.0012237070875641513 \n",
      "Epoch 81200/100000 Loss: 0.0012220896477934302 \n",
      "Epoch 81300/100000 Loss: 0.0012204764540702552 \n",
      "Epoch 81400/100000 Loss: 0.00121886748974472 \n",
      "Epoch 81500/100000 Loss: 0.0012172627382537602 \n",
      "Epoch 81600/100000 Loss: 0.0012156621831205668 \n",
      "Epoch 81700/100000 Loss: 0.0012140658079539202 \n",
      "Epoch 81800/100000 Loss: 0.0012124735964477707 \n",
      "Epoch 81900/100000 Loss: 0.001210885532380347 \n",
      "Epoch 82000/100000 Loss: 0.0012093015996142405 \n",
      "Epoch 82100/100000 Loss: 0.0012077217820953183 \n",
      "Epoch 82200/100000 Loss: 0.0012061460638523053 \n",
      "Epoch 82300/100000 Loss: 0.001204574428996533 \n",
      "Epoch 82400/100000 Loss: 0.0012030068617209936 \n",
      "Epoch 82500/100000 Loss: 0.0012014433463000324 \n",
      "Epoch 82600/100000 Loss: 0.0011998838670888188 \n",
      "Epoch 82700/100000 Loss: 0.001198328408522984 \n",
      "Epoch 82800/100000 Loss: 0.0011967769551175906 \n",
      "Epoch 82900/100000 Loss: 0.0011952294914673277 \n",
      "Epoch 83000/100000 Loss: 0.0011936860022454512 \n",
      "Epoch 83100/100000 Loss: 0.0011921464722035068 \n",
      "Epoch 83200/100000 Loss: 0.0011906108861707992 \n",
      "Epoch 83300/100000 Loss: 0.0011890792290538642 \n",
      "Epoch 83400/100000 Loss: 0.0011875514858360771 \n",
      "Epoch 83500/100000 Loss: 0.0011860276415771808 \n",
      "Epoch 83600/100000 Loss: 0.0011845076814125327 \n",
      "Epoch 83700/100000 Loss: 0.001182991590552909 \n",
      "Epoch 83800/100000 Loss: 0.001181479354284005 \n",
      "Epoch 83900/100000 Loss: 0.0011799709579657651 \n",
      "Epoch 84000/100000 Loss: 0.0011784663870321333 \n",
      "Epoch 84100/100000 Loss: 0.0011769656269904943 \n",
      "Epoch 84200/100000 Loss: 0.0011754686634212017 \n",
      "Epoch 84300/100000 Loss: 0.0011739754819770202 \n",
      "Epoch 84400/100000 Loss: 0.0011724860683829858 \n",
      "Epoch 84500/100000 Loss: 0.0011710004084356546 \n",
      "Epoch 84600/100000 Loss: 0.001169518488002823 \n",
      "Epoch 84700/100000 Loss: 0.0011680402930228891 \n",
      "Epoch 84800/100000 Loss: 0.0011665658095044884 \n",
      "Epoch 84900/100000 Loss: 0.0011650950235263273 \n",
      "Epoch 85000/100000 Loss: 0.0011636279212364315 \n",
      "Epoch 85100/100000 Loss: 0.0011621644888516447 \n",
      "Epoch 85200/100000 Loss: 0.0011607047126576276 \n",
      "Epoch 85300/100000 Loss: 0.0011592485790080228 \n",
      "Epoch 85400/100000 Loss: 0.001157796074324149 \n",
      "Epoch 85500/100000 Loss: 0.001156347185094666 \n",
      "Epoch 85600/100000 Loss: 0.001154901897875101 \n",
      "Epoch 85700/100000 Loss: 0.0011534601992873477 \n",
      "Epoch 85800/100000 Loss: 0.0011520220760196105 \n",
      "Epoch 85900/100000 Loss: 0.0011505875148255125 \n",
      "Epoch 86000/100000 Loss: 0.0011491565025238187 \n",
      "Epoch 86100/100000 Loss: 0.0011477290259984058 \n",
      "Epoch 86200/100000 Loss: 0.0011463050721974286 \n",
      "Epoch 86300/100000 Loss: 0.0011448846281330125 \n",
      "Epoch 86400/100000 Loss: 0.001143467680881225 \n",
      "Epoch 86500/100000 Loss: 0.0011420542175810755 \n",
      "Epoch 86600/100000 Loss: 0.0011406442254346513 \n",
      "Epoch 86700/100000 Loss: 0.0011392376917063961 \n",
      "Epoch 86800/100000 Loss: 0.001137834603722997 \n",
      "Epoch 86900/100000 Loss: 0.0011364349488728557 \n",
      "Epoch 87000/100000 Loss: 0.0011350387146056157 \n",
      "Epoch 87100/100000 Loss: 0.0011336458884320772 \n",
      "Epoch 87200/100000 Loss: 0.0011322564579235859 \n",
      "Epoch 87300/100000 Loss: 0.0011308704107115865 \n",
      "Epoch 87400/100000 Loss: 0.0011294877344876786 \n",
      "Epoch 87500/100000 Loss: 0.0011281084170028923 \n",
      "Epoch 87600/100000 Loss: 0.0011267324460674103 \n",
      "Epoch 87700/100000 Loss: 0.0011253598095500661 \n",
      "Epoch 87800/100000 Loss: 0.0011239904953785108 \n",
      "Epoch 87900/100000 Loss: 0.0011226244915380993 \n",
      "Epoch 88000/100000 Loss: 0.0011212617860723367 \n",
      "Epoch 88100/100000 Loss: 0.0011199023670817078 \n",
      "Epoch 88200/100000 Loss: 0.0011185462227242882 \n",
      "Epoch 88300/100000 Loss: 0.0011171933412143556 \n",
      "Epoch 88400/100000 Loss: 0.0011158437108229155 \n",
      "Epoch 88500/100000 Loss: 0.001114497319876895 \n",
      "Epoch 88600/100000 Loss: 0.0011131541567590312 \n",
      "Epoch 88700/100000 Loss: 0.001111814209907286 \n",
      "Epoch 88800/100000 Loss: 0.0011104774678148191 \n",
      "Epoch 88900/100000 Loss: 0.001109143919029402 \n",
      "Epoch 89000/100000 Loss: 0.0011078135521534185 \n",
      "Epoch 89100/100000 Loss: 0.001106486355843113 \n",
      "Epoch 89200/100000 Loss: 0.0011051623188086462 \n",
      "Epoch 89300/100000 Loss: 0.0011038414298134259 \n",
      "Epoch 89400/100000 Loss: 0.0011025236776742195 \n",
      "Epoch 89500/100000 Loss: 0.0011012090512604843 \n",
      "Epoch 89600/100000 Loss: 0.0010998975394942297 \n",
      "Epoch 89700/100000 Loss: 0.0010985891313495986 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89800/100000 Loss: 0.0010972838158528112 \n",
      "Epoch 89900/100000 Loss: 0.0010959815820813033 \n",
      "Epoch 90000/100000 Loss: 0.001094682419164142 \n",
      "Epoch 90100/100000 Loss: 0.00109338631628122 \n",
      "Epoch 90200/100000 Loss: 0.0010920932626631425 \n",
      "Epoch 90300/100000 Loss: 0.0010908032475910611 \n",
      "Epoch 90400/100000 Loss: 0.0010895162603958943 \n",
      "Epoch 90500/100000 Loss: 0.001088232290458549 \n",
      "Epoch 90600/100000 Loss: 0.001086951327209449 \n",
      "Epoch 90700/100000 Loss: 0.0010856733601282817 \n",
      "Epoch 90800/100000 Loss: 0.0010843983787436106 \n",
      "Epoch 90900/100000 Loss: 0.0010831263726325685 \n",
      "Epoch 91000/100000 Loss: 0.001081857331420856 \n",
      "Epoch 91100/100000 Loss: 0.0010805912447821304 \n",
      "Epoch 91200/100000 Loss: 0.0010793281024379221 \n",
      "Epoch 91300/100000 Loss: 0.0010780678941572162 \n",
      "Epoch 91400/100000 Loss: 0.0010768106097563965 \n",
      "Epoch 91500/100000 Loss: 0.00107555623909869 \n",
      "Epoch 91600/100000 Loss: 0.0010743047720942207 \n",
      "Epoch 91700/100000 Loss: 0.001073056198699343 \n",
      "Epoch 91800/100000 Loss: 0.0010718105089168352 \n",
      "Epoch 91900/100000 Loss: 0.0010705676927952328 \n",
      "Epoch 92000/100000 Loss: 0.0010693277404287985 \n",
      "Epoch 92100/100000 Loss: 0.001068090641957134 \n",
      "Epoch 92200/100000 Loss: 0.0010668563875651518 \n",
      "Epoch 92300/100000 Loss: 0.0010656249674822953 \n",
      "Epoch 92400/100000 Loss: 0.0010643963719830399 \n",
      "Epoch 92500/100000 Loss: 0.0010631705913860016 \n",
      "Epoch 92600/100000 Loss: 0.001061947616053883 \n",
      "Epoch 92700/100000 Loss: 0.0010607274363934151 \n",
      "Epoch 92800/100000 Loss: 0.0010595100428548304 \n",
      "Epoch 92900/100000 Loss: 0.001058295425931694 \n",
      "Epoch 93000/100000 Loss: 0.0010570835761609335 \n",
      "Epoch 93100/100000 Loss: 0.0010558744841219735 \n",
      "Epoch 93200/100000 Loss: 0.0010546681404372382 \n",
      "Epoch 93300/100000 Loss: 0.0010534645357713442 \n",
      "Epoch 93400/100000 Loss: 0.0010522636608311829 \n",
      "Epoch 93500/100000 Loss: 0.001051065506365503 \n",
      "Epoch 93600/100000 Loss: 0.0010498700631646058 \n",
      "Epoch 93700/100000 Loss: 0.0010486773220606486 \n",
      "Epoch 93800/100000 Loss: 0.001047487273926478 \n",
      "Epoch 93900/100000 Loss: 0.0010462999096764077 \n",
      "Epoch 94000/100000 Loss: 0.0010451152202650786 \n",
      "Epoch 94100/100000 Loss: 0.0010439331966881798 \n",
      "Epoch 94200/100000 Loss: 0.0010427538299811437 \n",
      "Epoch 94300/100000 Loss: 0.001041577111219867 \n",
      "Epoch 94400/100000 Loss: 0.0010404030315199879 \n",
      "Epoch 94500/100000 Loss: 0.0010392315820366635 \n",
      "Epoch 94600/100000 Loss: 0.0010380627539647081 \n",
      "Epoch 94700/100000 Loss: 0.0010368965385378155 \n",
      "Epoch 94800/100000 Loss: 0.0010357329270290292 \n",
      "Epoch 94900/100000 Loss: 0.0010345719107499383 \n",
      "Epoch 95000/100000 Loss: 0.001033413481050508 \n",
      "Epoch 95100/100000 Loss: 0.001032257629319415 \n",
      "Epoch 95200/100000 Loss: 0.0010311043469832112 \n",
      "Epoch 95300/100000 Loss: 0.0010299536255063808 \n",
      "Epoch 95400/100000 Loss: 0.001028805456391061 \n",
      "Epoch 95500/100000 Loss: 0.0010276598311772078 \n",
      "Epoch 95600/100000 Loss: 0.0010265167414415966 \n",
      "Epoch 95700/100000 Loss: 0.0010253761787984038 \n",
      "Epoch 95800/100000 Loss: 0.001024238134898569 \n",
      "Epoch 95900/100000 Loss: 0.0010231026014296536 \n",
      "Epoch 96000/100000 Loss: 0.001021969570115926 \n",
      "Epoch 96100/100000 Loss: 0.001020839032717692 \n",
      "Epoch 96200/100000 Loss: 0.0010197109810312968 \n",
      "Epoch 96300/100000 Loss: 0.0010185854068893445 \n",
      "Epoch 96400/100000 Loss: 0.0010174623021595602 \n",
      "Epoch 96500/100000 Loss: 0.0010163416587456778 \n",
      "Epoch 96600/100000 Loss: 0.0010152234685864407 \n",
      "Epoch 96700/100000 Loss: 0.0010141077236558215 \n",
      "Epoch 96800/100000 Loss: 0.0010129944159625506 \n",
      "Epoch 96900/100000 Loss: 0.001011883537550227 \n",
      "Epoch 97000/100000 Loss: 0.0010107750804970127 \n",
      "Epoch 97100/100000 Loss: 0.001009669036915297 \n",
      "Epoch 97200/100000 Loss: 0.0010085653989516977 \n",
      "Epoch 97300/100000 Loss: 0.0010074641587868947 \n",
      "Epoch 97400/100000 Loss: 0.001006365308635127 \n",
      "Epoch 97500/100000 Loss: 0.0010052688407443896 \n",
      "Epoch 97600/100000 Loss: 0.0010041747473962636 \n",
      "Epoch 97700/100000 Loss: 0.001003083020905363 \n",
      "Epoch 97800/100000 Loss: 0.0010019936536194436 \n",
      "Epoch 97900/100000 Loss: 0.001000906637919125 \n",
      "Epoch 98000/100000 Loss: 0.0009998219662179187 \n",
      "Epoch 98100/100000 Loss: 0.0009987396309615604 \n",
      "Epoch 98200/100000 Loss: 0.000997659624628482 \n",
      "Epoch 98300/100000 Loss: 0.0009965819397291448 \n",
      "Epoch 98400/100000 Loss: 0.000995506568806094 \n",
      "Epoch 98500/100000 Loss: 0.0009944335044336818 \n",
      "Epoch 98600/100000 Loss: 0.000993362739217898 \n",
      "Epoch 98700/100000 Loss: 0.0009922942657964282 \n",
      "Epoch 98800/100000 Loss: 0.0009912280768382346 \n",
      "Epoch 98900/100000 Loss: 0.0009901641650433063 \n",
      "Epoch 99000/100000 Loss: 0.0009891025231427695 \n",
      "Epoch 99100/100000 Loss: 0.0009880431438987222 \n",
      "Epoch 99200/100000 Loss: 0.0009869860201038422 \n",
      "Epoch 99300/100000 Loss: 0.0009859311445811671 \n",
      "Epoch 99400/100000 Loss: 0.0009848785101844258 \n",
      "Epoch 99500/100000 Loss: 0.0009838281097974272 \n",
      "Epoch 99600/100000 Loss: 0.0009827799363338104 \n",
      "Epoch 99700/100000 Loss: 0.0009817339827373212 \n",
      "Epoch 99800/100000 Loss: 0.0009806902419815635 \n",
      "Epoch 99900/100000 Loss: 0.0009796487070694133 \n",
      "Epoch 100000/100000 Loss: 0.0009786093710333252 \n",
      "W11(1):  7.9222404813756535  W21(1):  7.956320102829322\n",
      "b1(1) -3.6748568093366734\n",
      "W12(1):  6.1358092385785294  W21(1):  6.142056007032763\n",
      "b2(1) -9.37777127698554\n",
      "W12(1):  14.878747475302216  W21(1):  -15.664535045253738\n",
      "b1(2) -7.042990617874711\n",
      "Predictions:\n",
      "[0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights and biases with random values\n",
    "        np.random.seed(42)\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.b1 = np.random.randn(1, self.hidden_size)\n",
    "        \n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.b2 = np.random.randn(1, self.output_size)\n",
    "        #self.W1 = np.array([[0.16,-0.07],[-0.66,0.63]])\n",
    "        #self.b1 = np.array([[0.37,-0.98]])\n",
    "        #self.W2 = np.array([[0.41],[-0.9]])\n",
    "        #self.b2 = np.array([[0.35]])\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Compute activations of the hidden layer\n",
    "        self.hidden_activations = self.sigmoid(np.dot(X, self.W1) + self.b1)\n",
    "        \n",
    "        # Compute activations of the output layers\n",
    "        self.output_activations = self.sigmoid(np.dot(self.hidden_activations, self.W2) + self.b2)\n",
    "        \n",
    "        return self.output_activations\n",
    "    \n",
    "    def backward(self, X, y, learning_rate):\n",
    "        m = X.shape[0]  # Number of training examples\n",
    "        \n",
    "        # Compute gradients\n",
    "        dZ2 = self.output_activations - y\n",
    "        #print(dZ2)\n",
    "        dW2 = (1 / m) * np.dot(self.hidden_activations.T, dZ2)\n",
    "        db2 = (1 / m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "        \n",
    "        #print(self.hidden_activations)\n",
    "        dZ1 = np.dot(dZ2, self.W2.T) * (self.hidden_activations * (1 - self.hidden_activations))\n",
    "        #print(dZ1)\n",
    "        dW1 = (1 / m) * np.dot(X.T, dZ1)\n",
    "        db1 = (1 / m) * np.sum(dZ1, axis=0)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        \n",
    "    \n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward propagation\n",
    "            output = self.forward(X)\n",
    "            m = len(y)\n",
    "            # Backward propagation\n",
    "            self.backward(X, y, learning_rate)\n",
    "            #rint(W1,b1,W2,b2)\n",
    "            # Print the loss every 100 epochs\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                loss = -1/m * np.sum((y * np.log(output) + (1 - y) * np.log(1 - output)))\n",
    "                #print(loss)\n",
    "                print(f\"Epoch {epoch + 1}/{epochs} Loss: {loss} \")\n",
    "\n",
    "    def predict(self, X):\n",
    "        print(\"W11(1): \",self.W1[0][0], \" W21(1): \",self.W1[1][0])\n",
    "        print(\"b1(1)\",self.b1[0][0])\n",
    "        print(\"W12(1): \",self.W1[0][1], \" W21(1): \",self.W1[1][1])\n",
    "        print(\"b2(1)\",self.b1[0][1])\n",
    "        print(\"W12(1): \",self.W2[0][0], \" W21(1): \",self.W2[1][0])\n",
    "        print(\"b1(2)\",self.b2[0][0])\n",
    "        predictions = [1 if pred > 0.5 else 0 for pred in self.forward(X)]\n",
    "        \n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "nn = NeuralNetwork(input_size=2, hidden_size=2, output_size=1)\n",
    "\n",
    "X = np.array([[0,0],[1,1],[1,0],[0,1]])\n",
    "y = np.array([[0],[0],[1],[1]])\n",
    "# Train the neural network\n",
    "nn.train(X, y, epochs=100000, learning_rate=0.1)\n",
    "\n",
    "# Make predictions on the input data\n",
    "predictions = nn.predict(X)\n",
    "print(\"Predictions:\")\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
